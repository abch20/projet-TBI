{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006dcb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Users/HP EliteBook/AppData/Local/Programs/Python/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Advanced Database Analysis\n",
    "# Schema: Dim_Client_final, Fait_Commande, Dim_Temps1, Dim_Employees_final\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "# Load your data (adjust paths and connection as needed)\n",
    "# Option 1: From CSV files\n",
    "df_clients = pd.read_csv('Data\\Dim_Client_final.csv')\n",
    "df_commandes = pd.read_csv('Data\\Fait Commande.csv')\n",
    "df_temps = pd.read_csv('Data\\Dim_Temps1.csv')\n",
    "df_employees = pd.read_csv('Data\\Dim_Employees_final.csv')\n",
    "\n",
    "# Option 2: From database\n",
    "# import sqlalchemy\n",
    "# engine = sqlalchemy.create_engine('your_connection_string')\n",
    "# df_clients = pd.read_sql_table('Dim_Client_final', engine)\n",
    "# df_commandes = pd.read_sql_table('Fait_Commande', engine)\n",
    "# df_temps = pd.read_sql_table('Dim_Temps1', engine)\n",
    "# df_employees = pd.read_sql_table('Dim_Employees_final', engine)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "def explore_data(df, name):\n",
    "    \"\"\"Comprehensive data exploration\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPLORING: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"\\nBasic Statistics:\\n{df.describe()}\")\n",
    "    print(f\"\\nFirst 5 rows:\\n{df.head()}\")\n",
    "\n",
    "# Uncomment to explore each table\n",
    "# explore_data(df_clients, \"Dim_Client_final\")\n",
    "# explore_data(df_commandes, \"Fait_Commande\")\n",
    "# explore_data(df_temps, \"Dim_Temps1\")\n",
    "# explore_data(df_employees, \"Dim_Employees_final\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATA MERGING & ENRICHMENT\n",
    "# ============================================================================\n",
    "\n",
    "def create_master_dataset(df_commandes, df_clients, df_employees, df_temps):\n",
    "    \"\"\"Create a comprehensive dataset by joining all tables\"\"\"\n",
    "    \n",
    "    # Join commandes with clients\n",
    "    df_master = df_commandes.merge(\n",
    "        df_clients,\n",
    "        left_on='id_seqClient',\n",
    "        right_on='id_seqClient',\n",
    "        how='left',\n",
    "        suffixes=('', '_client')\n",
    "    )\n",
    "    \n",
    "    # Join with employees\n",
    "    df_master = df_master.merge(\n",
    "        df_employees,\n",
    "        left_on='id_seq_Employee',\n",
    "        right_on='id_seq_Employee',\n",
    "        how='left',\n",
    "        suffixes=('', '_employee')\n",
    "    )\n",
    "    \n",
    "    # Join with time dimension\n",
    "    df_master = df_master.merge(\n",
    "        df_temps,\n",
    "        left_on='id_temps',\n",
    "        right_on='id_temps',\n",
    "        how='left',\n",
    "        suffixes=('', '_temps')\n",
    "    )\n",
    "    \n",
    "    return df_master\n",
    "\n",
    "df_master = create_master_dataset(df_commandes, df_clients, df_employees, df_temps)\n",
    "print(f\"Master dataset shape: {df_master.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUSINESS METRICS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_kpis(df_master):\n",
    "    \"\"\"Calculate key performance indicators\"\"\"\n",
    "    \n",
    "    kpis = {\n",
    "        'total_orders': len(df_master),\n",
    "        'total_delivered': df_master['Nbr_Commande_Livree'].sum(),\n",
    "        'total_undelivered': df_master['Nbr_Commande_Non_Livree'].sum(),\n",
    "        'delivery_rate': (df_master['Nbr_Commande_Livree'].sum() / \n",
    "                         (df_master['Nbr_Commande_Livree'].sum() + \n",
    "                          df_master['Nbr_Commande_Non_Livree'].sum()) * 100),\n",
    "        'unique_customers': df_master['Customer ID'].nunique(),\n",
    "        'unique_employees': df_master['Employee ID'].nunique(),\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([kpis])\n",
    "\n",
    "kpis = calculate_kpis(df_master)\n",
    "print(\"\\nKEY PERFORMANCE INDICATORS:\")\n",
    "print(kpis.T)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CUSTOMER ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def customer_analysis(df_master):\n",
    "    \"\"\"Analyze customer behavior and segmentation\"\"\"\n",
    "    \n",
    "    # Customer order frequency\n",
    "    customer_orders = df_master.groupby('Customer ID').agg({\n",
    "        'id_seq_fait': 'count',\n",
    "        'Nbr_Commande_Livree': 'sum',\n",
    "        'Nbr_Commande_Non_Livree': 'sum',\n",
    "        'Country': 'first',\n",
    "        'City': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_orders.columns = ['Customer_ID', 'Total_Orders', 'Delivered', \n",
    "                               'Undelivered', 'Country', 'City']\n",
    "    \n",
    "    # Calculate delivery success rate per customer\n",
    "    customer_orders['Delivery_Rate'] = (\n",
    "        customer_orders['Delivered'] / \n",
    "        (customer_orders['Delivered'] + customer_orders['Undelivered']) * 100\n",
    "    )\n",
    "    \n",
    "    # Customer segmentation by order volume\n",
    "    customer_orders['Segment'] = pd.cut(\n",
    "        customer_orders['Total_Orders'],\n",
    "        bins=[0, 1, 5, 10, float('inf')],\n",
    "        labels=['One-time', 'Occasional', 'Regular', 'VIP']\n",
    "    )\n",
    "    \n",
    "    return customer_orders\n",
    "\n",
    "customer_stats = customer_analysis(df_master)\n",
    "print(\"\\nCUSTOMER SEGMENTS:\")\n",
    "print(customer_stats['Segment'].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# 6. EMPLOYEE PERFORMANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def employee_performance(df_master):\n",
    "    \"\"\"Analyze employee performance metrics\"\"\"\n",
    "    \n",
    "    employee_stats = df_master.groupby(['Employee ID', 'Nom', 'Prenom']).agg({\n",
    "        'id_seq_fait': 'count',\n",
    "        'Nbr_Commande_Livree': 'sum',\n",
    "        'Nbr_Commande_Non_Livree': 'sum',\n",
    "        'Region': 'first',\n",
    "        'Territory': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    employee_stats.columns = ['Employee_ID', 'Last_Name', 'First_Name', \n",
    "                              'Total_Orders', 'Delivered', 'Undelivered',\n",
    "                              'Region', 'Territory']\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    employee_stats['Delivery_Rate'] = (\n",
    "        employee_stats['Delivered'] / \n",
    "        (employee_stats['Delivered'] + employee_stats['Undelivered']) * 100\n",
    "    )\n",
    "    \n",
    "    employee_stats['Efficiency_Score'] = (\n",
    "        employee_stats['Delivered'] / employee_stats['Total_Orders'] * 100\n",
    "    )\n",
    "    \n",
    "    # Rank employees\n",
    "    employee_stats['Rank'] = employee_stats['Delivery_Rate'].rank(\n",
    "        ascending=False, method='dense'\n",
    "    )\n",
    "    \n",
    "    return employee_stats.sort_values('Delivery_Rate', ascending=False)\n",
    "\n",
    "employee_perf = employee_performance(df_master)\n",
    "print(\"\\nTOP 10 EMPLOYEES BY DELIVERY RATE:\")\n",
    "print(employee_perf.head(10))\n",
    "\n",
    "# ============================================================================\n",
    "# 7. TEMPORAL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def temporal_analysis(df_master):\n",
    "    \"\"\"Analyze trends over time\"\"\"\n",
    "    \n",
    "    # Assuming df_temps has date-related columns (Annee, mois_annee, etc.)\n",
    "    temporal = df_master.groupby(['Annee', 'mois_annee']).agg({\n",
    "        'id_seq_fait': 'count',\n",
    "        'Nbr_Commande_Livree': 'sum',\n",
    "        'Nbr_Commande_Non_Livree': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    temporal.columns = ['Year', 'Month', 'Total_Orders', \n",
    "                        'Delivered', 'Undelivered']\n",
    "    \n",
    "    temporal['Delivery_Rate'] = (\n",
    "        temporal['Delivered'] / \n",
    "        (temporal['Delivered'] + temporal['Undelivered']) * 100\n",
    "    )\n",
    "    \n",
    "    return temporal\n",
    "\n",
    "temporal_trends = temporal_analysis(df_master)\n",
    "print(\"\\nTEMPORAL TRENDS:\")\n",
    "print(temporal_trends.tail(12))\n",
    "\n",
    "# ============================================================================\n",
    "# 8. GEOGRAPHICAL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def geographical_analysis(df_master):\n",
    "    \"\"\"Analyze performance by geography\"\"\"\n",
    "    \n",
    "    # By Country\n",
    "    country_stats = df_master.groupby('Country').agg({\n",
    "        'id_seq_fait': 'count',\n",
    "        'Nbr_Commande_Livree': 'sum',\n",
    "        'Nbr_Commande_Non_Livree': 'sum',\n",
    "        'Customer ID': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    country_stats.columns = ['Country', 'Total_Orders', 'Delivered', \n",
    "                             'Undelivered', 'Unique_Customers']\n",
    "    \n",
    "    country_stats['Delivery_Rate'] = (\n",
    "        country_stats['Delivered'] / \n",
    "        (country_stats['Delivered'] + country_stats['Undelivered']) * 100\n",
    "    )\n",
    "    \n",
    "    return country_stats.sort_values('Total_Orders', ascending=False)\n",
    "\n",
    "geo_stats = geographical_analysis(df_master)\n",
    "print(\"\\nGEOGRAPHICAL PERFORMANCE:\")\n",
    "print(geo_stats)\n",
    "\n",
    "# ============================================================================\n",
    "# 9. ADVANCED VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_visualizations(df_master):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Delivery Rate by Country\n",
    "    country_data = df_master.groupby('Country').agg({\n",
    "        'Nbr_Commande_Livree': 'sum',\n",
    "        'Nbr_Commande_Non_Livree': 'sum'\n",
    "    })\n",
    "    country_data['Total'] = country_data.sum(axis=1)\n",
    "    country_data['Delivery_Rate'] = (\n",
    "        country_data['Nbr_Commande_Livree'] / country_data['Total'] * 100\n",
    "    )\n",
    "    country_data.nlargest(10, 'Total')['Delivery_Rate'].plot(\n",
    "        kind='barh', ax=axes[0, 0], color='steelblue'\n",
    "    )\n",
    "    axes[0, 0].set_title('Top 10 Countries by Delivery Rate', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Delivery Rate (%)')\n",
    "    \n",
    "    # 2. Orders Over Time\n",
    "    temporal = df_master.groupby('mois_annee')['id_seq_fait'].count()\n",
    "    temporal.plot(kind='line', ax=axes[0, 1], marker='o', color='darkgreen')\n",
    "    axes[0, 1].set_title('Order Trends Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Number of Orders')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Employee Performance Distribution\n",
    "    emp_perf = df_master.groupby('Employee ID').agg({\n",
    "        'Nbr_Commande_Livree': 'sum',\n",
    "        'Nbr_Commande_Non_Livree': 'sum'\n",
    "    })\n",
    "    emp_perf['Total'] = emp_perf.sum(axis=1)\n",
    "    emp_perf['Total'].plot(kind='hist', bins=20, ax=axes[1, 0], \n",
    "                           color='coral', edgecolor='black')\n",
    "    axes[1, 0].set_title('Distribution of Orders per Employee', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Number of Orders')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 4. Delivered vs Undelivered\n",
    "    delivered = df_master['Nbr_Commande_Livree'].sum()\n",
    "    undelivered = df_master['Nbr_Commande_Non_Livree'].sum()\n",
    "    axes[1, 1].pie([delivered, undelivered], \n",
    "                   labels=['Delivered', 'Undelivered'],\n",
    "                   autopct='%1.1f%%', startangle=90,\n",
    "                   colors=['#2ecc71', '#e74c3c'])\n",
    "    axes[1, 1].set_title('Overall Delivery Status', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('overview_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "create_visualizations(df_master)\n",
    "\n",
    "# ============================================================================\n",
    "# 10. CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def correlation_analysis(df_master):\n",
    "    \"\"\"Analyze correlations between numerical variables\"\"\"\n",
    "    \n",
    "    # Select numerical columns\n",
    "    numerical_cols = df_master.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = df_master[numerical_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, fmt='.2f')\n",
    "    plt.title('Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "corr_matrix = correlation_analysis(df_master)\n",
    "\n",
    "# ============================================================================\n",
    "# 11. RFM ANALYSIS (Recency, Frequency, Monetary)\n",
    "# ============================================================================\n",
    "\n",
    "def rfm_analysis(df_master):\n",
    "    \"\"\"Customer segmentation using RFM analysis\"\"\"\n",
    "    \n",
    "    # Detect available date columns and use the best one\n",
    "    date_columns = ['Order_Date', 'date', 'Date', 'Annee', 'Year']\n",
    "    date_col = None\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_master.columns:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    if date_col is None:\n",
    "        print(\"Warning: No date column found. Using 'Annee' for recency calculation\")\n",
    "        date_col = 'Annee'\n",
    "    \n",
    "    # Calculate snapshot date (most recent date in dataset)\n",
    "    try:\n",
    "        snapshot_date = df_master[date_col].max()\n",
    "    except:\n",
    "        snapshot_date = df_master['Annee'].max()\n",
    "        date_col = 'Annee'\n",
    "    \n",
    "    print(f\"Using '{date_col}' for recency calculation\")\n",
    "    print(f\"Snapshot date: {snapshot_date}\")\n",
    "    \n",
    "    # Calculate RFM metrics\n",
    "    rfm = df_master.groupby('Customer ID').agg({\n",
    "        date_col: lambda x: snapshot_date - x.max(),  # Recency\n",
    "        'id_seq_fait': 'count',  # Frequency\n",
    "        'Nbr_Commande_Livree': 'sum'  # Monetary (using delivered orders as proxy)\n",
    "    }).reset_index()\n",
    "    \n",
    "    rfm.columns = ['Customer_ID', 'Recency', 'Frequency', 'Monetary']\n",
    "    \n",
    "    print(f\"\\nRFM Summary Statistics:\")\n",
    "    print(rfm.describe())\n",
    "    \n",
    "    # Create RFM scores (1-5) with duplicate handling\n",
    "    try:\n",
    "        rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5, 4, 3, 2, 1], duplicates='drop')\n",
    "    except (ValueError, TypeError):\n",
    "        # If qcut fails, use cut with manual bins or percentile-based scoring\n",
    "        try:\n",
    "            rfm['R_Score'] = pd.cut(rfm['Recency'], \n",
    "                                    bins=5, \n",
    "                                    labels=[5, 4, 3, 2, 1], \n",
    "                                    duplicates='drop')\n",
    "        except:\n",
    "            # Last resort: use rank-based scoring\n",
    "            rfm['R_Score'] = 6 - pd.qcut(rfm['Recency'].rank(method='first'), \n",
    "                                         5, \n",
    "                                         labels=[1, 2, 3, 4, 5],\n",
    "                                         duplicates='drop').astype(int)\n",
    "    \n",
    "    try:\n",
    "        rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, \n",
    "                                 labels=[1, 2, 3, 4, 5], \n",
    "                                 duplicates='drop')\n",
    "    except (ValueError, TypeError):\n",
    "        try:\n",
    "            rfm['F_Score'] = pd.cut(rfm['Frequency'], \n",
    "                                    bins=5, \n",
    "                                    labels=[1, 2, 3, 4, 5], \n",
    "                                    duplicates='drop')\n",
    "        except:\n",
    "            # Use percentile-based scoring\n",
    "            rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), \n",
    "                                     q=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                     labels=[1, 2, 3, 4, 5],\n",
    "                                     duplicates='drop')\n",
    "    \n",
    "    try:\n",
    "        rfm['M_Score'] = pd.qcut(rfm['Monetary'].rank(method='first'), 5, \n",
    "                                 labels=[1, 2, 3, 4, 5], \n",
    "                                 duplicates='drop')\n",
    "    except (ValueError, TypeError):\n",
    "        try:\n",
    "            rfm['M_Score'] = pd.cut(rfm['Monetary'], \n",
    "                                    bins=5, \n",
    "                                    labels=[1, 2, 3, 4, 5], \n",
    "                                    duplicates='drop')\n",
    "        except:\n",
    "            rfm['M_Score'] = pd.qcut(rfm['Monetary'].rank(method='first'), \n",
    "                                     q=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                     labels=[1, 2, 3, 4, 5],\n",
    "                                     duplicates='drop')\n",
    "    \n",
    "    # Handle any NaN scores\n",
    "    rfm['R_Score'] = rfm['R_Score'].fillna(3).astype(int)\n",
    "    rfm['F_Score'] = rfm['F_Score'].fillna(3).astype(int)\n",
    "    rfm['M_Score'] = rfm['M_Score'].fillna(3).astype(int)\n",
    "    \n",
    "    # Calculate RFM score\n",
    "    rfm['RFM_Score'] = rfm['R_Score'] + rfm['F_Score'] + rfm['M_Score']\n",
    "    \n",
    "    # Segment customers\n",
    "    try:\n",
    "        rfm['Segment'] = pd.cut(rfm['RFM_Score'], \n",
    "                                bins=[0, 6, 9, 12, 15],\n",
    "                                labels=['At Risk', 'Developing', 'Loyal', 'Champions'],\n",
    "                                include_lowest=True)\n",
    "    except:\n",
    "        # Alternative: Use quartiles for segmentation\n",
    "        rfm['Segment'] = pd.qcut(rfm['RFM_Score'],\n",
    "                                 q=4,\n",
    "                                 labels=['At Risk', 'Developing', 'Loyal', 'Champions'],\n",
    "                                 duplicates='drop')\n",
    "    \n",
    "    # Handle any remaining NaN in Segment\n",
    "    rfm['Segment'] = rfm['Segment'].fillna('Developing')\n",
    "    \n",
    "    print(f\"\\nRFM Segmentation Complete!\")\n",
    "    print(f\"Segments distribution:\")\n",
    "    print(rfm['Segment'].value_counts())\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "rfm_data = rfm_analysis(df_master)\n",
    "print(\"\\nRFM SEGMENTATION:\")\n",
    "print(rfm_data['Segment'].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# 12. PREDICTIVE MODELING (Delivery Success)\n",
    "# ============================================================================\n",
    "\n",
    "def build_delivery_prediction_model(df_master):\n",
    "    \"\"\"Build a model to predict delivery success\"\"\"\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Prepare features\n",
    "    df_model = df_master.copy()\n",
    "    \n",
    "    # Create target variable (1 if delivered, 0 if not)\n",
    "    df_model['Delivered_Success'] = (\n",
    "        df_model['Nbr_Commande_Livree'] > df_model['Nbr_Commande_Non_Livree']\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = ['Employee ID', 'Region', 'Territory', 'Country', \n",
    "                    'City', 'Annee', 'mois_annee']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_dict = {}\n",
    "    for col in feature_cols:\n",
    "        if df_model[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            df_model[col + '_encoded'] = le.fit_transform(df_model[col].astype(str))\n",
    "            le_dict[col] = le\n",
    "    \n",
    "    # Select encoded features\n",
    "    encoded_features = [col + '_encoded' if df_model[col].dtype == 'object' \n",
    "                       else col for col in feature_cols]\n",
    "    \n",
    "    X = df_model[encoded_features].fillna(0)\n",
    "    y = df_model['Delivered_Success']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\nMODEL PERFORMANCE:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': encoded_features,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance.head(10), x='Importance', y='Feature')\n",
    "    plt.title('Top 10 Feature Importance for Delivery Success', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return rf_model, feature_importance\n",
    "\n",
    "model, feature_imp = build_delivery_prediction_model(df_master)\n",
    "\n",
    "# ============================================================================\n",
    "# 13. COHORT ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def cohort_analysis(df_master):\n",
    "    \"\"\"Analyze customer retention and behavior over time\"\"\"\n",
    "    \n",
    "    # Assuming you have proper date columns\n",
    "    # df_master['Order_Date'] = pd.to_datetime(df_master['date_column'])\n",
    "    \n",
    "    # Create cohort month (first order month for each customer)\n",
    "    df_master['Cohort'] = df_master.groupby('Customer ID')['Annee'].transform('min')\n",
    "    \n",
    "    # Count orders by cohort and period\n",
    "    cohort_data = df_master.groupby(['Cohort', 'Annee']).agg({\n",
    "        'Customer ID': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    cohort_pivot = cohort_data.pivot(index='Cohort', \n",
    "                                      columns='Annee', \n",
    "                                      values='Customer ID')\n",
    "    \n",
    "    # Calculate retention rates\n",
    "    cohort_size = cohort_pivot.iloc[:, 0]\n",
    "    retention = cohort_pivot.divide(cohort_size, axis=0) * 100\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(retention, annot=True, fmt='.0f', cmap='YlGnBu')\n",
    "    plt.title('Customer Retention Rate by Cohort (%)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Cohort (First Order Year)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cohort_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return retention\n",
    "\n",
    "retention_rates = cohort_analysis(df_master)\n",
    "\n",
    "# ============================================================================\n",
    "# 14. ANOMALY DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def detect_anomalies(df_master):\n",
    "    \"\"\"Detect unusual patterns in orders\"\"\"\n",
    "    \n",
    "    from scipy import stats\n",
    "\n",
    "    # Calculate z-scores for delivered orders\n",
    "    df_master['Delivered_ZScore'] = stats.zscore(\n",
    "        df_master['Nbr_Commande_Livree'].fillna(0)\n",
    "    )\n",
    "    \n",
    "    # Flag anomalies (z-score > 3 or < -3)\n",
    "    df_master['Is_Anomaly'] = (\n",
    "        abs(df_master['Delivered_ZScore']) > 3\n",
    "    )\n",
    "    \n",
    "    anomalies = df_master[df_master['Is_Anomaly']]\n",
    "    \n",
    "    print(f\"\\nANOMALIES DETECTED: {len(anomalies)}\")\n",
    "    print(\"\\nAnomaly Summary:\")\n",
    "    print(anomalies[['Customer ID', 'Employee ID', 'Country', \n",
    "                     'Nbr_Commande_Livree', 'Delivered_ZScore']].head(10))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(df_master.index, df_master['Nbr_Commande_Livree'], \n",
    "                c=df_master['Is_Anomaly'], cmap='coolwarm', alpha=0.6)\n",
    "    plt.title('Anomaly Detection in Delivered Orders', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Number of Delivered Orders')\n",
    "    plt.xlabel('Order Index')\n",
    "    plt.colorbar(label='Anomaly')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('anomaly_detection.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "anomalies = detect_anomalies(df_master)\n",
    "\n",
    "# ============================================================================\n",
    "# 15. EXPORT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "def export_analysis_results(customer_stats, employee_perf, geo_stats, rfm_data):\n",
    "    \"\"\"Export all analysis results to CSV\"\"\"\n",
    "    \n",
    "    customer_stats.to_csv('customer_analysis.csv', index=False)\n",
    "    employee_perf.to_csv('employee_performance.csv', index=False)\n",
    "    geo_stats.to_csv('geographical_analysis.csv', index=False)\n",
    "    rfm_data.to_csv('rfm_segmentation.csv', index=False)\n",
    "    \n",
    "    print(\"\\nAll analysis results exported successfully!\")\n",
    "    print(\"Files created:\")\n",
    "    print(\"- customer_analysis.csv\")\n",
    "    print(\"- employee_performance.csv\")\n",
    "    print(\"- geographical_analysis.csv\")\n",
    "    print(\"- rfm_segmentation.csv\")\n",
    "\n",
    "export_analysis_results(customer_stats, employee_perf, geo_stats, rfm_data)\n",
    "\n",
    "# ============================================================================\n",
    "# 16. COMPREHENSIVE REPORT GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "def generate_executive_summary(df_master):\n",
    "    \"\"\"Generate an executive summary report\"\"\"\n",
    "    \n",
    "    summary = f\"\"\"\n",
    "    {'='*70}\n",
    "    EXECUTIVE SUMMARY - ADVANCED DATA ANALYSIS\n",
    "    {'='*70}\n",
    "    \n",
    "    OVERVIEW:\n",
    "    - Total Orders: {len(df_master):,}\n",
    "    - Total Delivered: {df_master['Nbr_Commande_Livree'].sum():,}\n",
    "    - Total Undelivered: {df_master['Nbr_Commande_Non_Livree'].sum():,}\n",
    "    - Overall Delivery Rate: {(df_master['Nbr_Commande_Livree'].sum() / (df_master['Nbr_Commande_Livree'].sum() + df_master['Nbr_Commande_Non_Livree'].sum()) * 100):.2f}%\n",
    "    \n",
    "    CUSTOMER INSIGHTS:\n",
    "    - Unique Customers: {df_master['Customer ID'].nunique():,}\n",
    "    - Countries Served: {df_master['Country'].nunique()}\n",
    "    - Cities Served: {df_master['City'].nunique()}\n",
    "    \n",
    "    EMPLOYEE INSIGHTS:\n",
    "    - Active Employees: {df_master['Employee ID'].nunique()}\n",
    "    - Regions Covered: {df_master['Region'].nunique()}\n",
    "    - Territories: {df_master['Territory'].nunique()}\n",
    "    \n",
    "    TOP PERFORMING COUNTRY:\n",
    "    {df_master.groupby('Country')['id_seq_fait'].count().nlargest(1).to_string()}\n",
    "    \n",
    "    {'='*70}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(summary)\n",
    "    \n",
    "    # Save to file\n",
    "    with open('executive_summary.txt', 'w') as f:\n",
    "        f.write(summary)\n",
    "\n",
    "generate_executive_summary(df_master)\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK START GUIDE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘          JUPYTER NOTEBOOK - ADVANCED ANALYSIS READY!               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "STEP 1: Load your CSV files\n",
    "   Uncomment lines 22-25 and adjust the file paths\n",
    "\n",
    "STEP 2: Run data exploration\n",
    "   Uncomment lines 52-55\n",
    "\n",
    "STEP 3: Create master dataset\n",
    "   Uncomment line 85\n",
    "\n",
    "STEP 4: Run analyses (uncomment as needed):\n",
    "   - KPIs: Line 108\n",
    "   - Customer Analysis: Line 131\n",
    "   - Employee Performance: Line 166\n",
    "   - Temporal Analysis: Line 189\n",
    "   - Geographical Analysis: Line 219\n",
    "   - Visualizations: Line 311\n",
    "   - RFM Analysis: Line 373\n",
    "   - Predictive Model: Line 463\n",
    "   - Anomaly Detection: Line 549\n",
    "\n",
    "STEP 5: Export results\n",
    "   Uncomment line 577\n",
    "\n",
    "Happy Analyzing! ğŸ“Š\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
